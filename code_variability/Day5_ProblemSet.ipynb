{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../resources/cropped-SummerWorkshop_Header.png\">  \n",
    "\n",
    "<h1 align=\"center\">Workshop SWDB 2023 </h1> \n",
    "<h3 align=\"center\">Friday, August 25th, 2023</h3>\n",
    "<h3 align=\"center\">Problem Set</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "<h2> 1. Consider other ways by which we can encode features in the models. Instead of using categorical one-hot encoding (which is an 8-dimensional vector) for stimulus orientation, we may use the scalar value of orientation directly as the regressor. What are potential benefits and limitations of different feature encoding methods? </h2>\n",
    "\n",
    "<p>\n",
    "    \n",
    "<li> Fit the regression models introduced in the worskshop, but using the scalar orientation value for the stimulus regressor. Can they capture the orientation/ direction selectivity potentially demonstrated by the neurons?\n",
    "<li> Compare and contrast the models you get with the ones in the workshop. Can you speculate what leads to the difference?\n",
    "<li> Discuss the pros and cons of this approach. What do you think should be considered when deciding on how to encode a feature in a regression model?\n",
    "<li> (solution provided)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "<h2> 2. Consider nonlinear relationships between neural responses and features. How can we extend the regression model to capture this relationship? Some examples include: regression with sine/ cosine basis of orientation value, polynomials basis, or parametrized nonlinear function (e.g. Von Mises). How do they compare and contrast? </h2>\n",
    "\n",
    "<p>\n",
    "    \n",
    "<li> Review materials on polynomial regression introduced on Day 1. How do you modify it to encode orientations? (Hint: orientation is circular variable which repeats itself after $[0, 2 \\pi]$. Consider sine/cosine functions, or <a href=\"https://en.wikipedia.org/wiki/Von_Mises_distribution\">Von Mises function </a>\n",
    "<li> Run regression models with nonlinear bases of orientation. What does it tell us about how the neurons are driven by stimulus?\n",
    "\n",
    "<li> Compare and contrast models with nonlinear bases with the ones we have in thr tutorial. What are some benefits and limitations of this approach?\n",
    "\n",
    "<li> (solution provided)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "<h2> 3. Linear regression models assume Gaussian noise for the data. Do you think this assumption applies to the data set? Consider models with non-Gaussian noise/ non-MSE loss. Do they explain more variability? </h2>\n",
    "\n",
    "<p>\n",
    "    \n",
    "<li> Generalized Linear Model (GLM) is a modeling framework for non-Gaussian noise. Check the <a href=\"https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-models\">scikit-learn GLM package</a> for introduction to the model, implementation, and use of the library.\n",
    "\n",
    "<li> To properly set up the design matrix, think thoroughly on how to encode the orientation stimulus (both its value and its time course)\n",
    "\n",
    "<li> Examine the filters derived from the fitted GLM. Do they tell us anything about what is driving the neural responses and how? How does it compare to our previous analyses using tuning curves, linear regression, and other methods?\n",
    "\n",
    "<li> Include inter-neuron interactions in the GLM. What does it tell us about the influence of one neuron on another? \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "<h2> 4. Try modeling with even more neurons per area, and with more brain areas. As we include more neurons in the model, we inevitably run into issues with a large parameter space (the curse of dimensionality). Consider applying dimensionality reduction techniques to reduce the neuron space into smaller latent space. How can we visualized and interpret the models? </h2>\n",
    "\n",
    "<p>\n",
    "    \n",
    "<li> Apply dimensionality reduction methods (e.g. Principal Component Analysis introduced on Day 2) to each brain area.\n",
    "\n",
    "<li> Fit regression models with the reduced components derived for each brain area.\n",
    "\n",
    "<li> Visualize the functional connectivity among different brain areas.\n",
    "\n",
    "<li> ** Consider methods that jointly reduce dimensionality and generate predictions. For example, Reduced Rank Regression (RRR) fit a regression model and constraint the dimensionality of the weight matrix at the same time. (the so-called \"communication subspace\" - find more information <a href=\"https://www.sciencedirect.com/science/article/pii/S0896627319300534\">here</a>)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "<h2> 5. Regression models require careful choice of bases to be effective. Another (increasingly popular) approach would be using recurrent neural network (RNN) to build dynamical models for neural responses. Try implementing RNN models for predicting neural responses. What do you think would be a good network architecture choice? Why? How could you verify? </h2>\n",
    "\n",
    "<p>\n",
    "    \n",
    "<li> In fact Model 6 from the workshop could be seen as an extremely simple fully linear RNN without nonlinear activation. Conisder starting by adding nonlinear activation function to the model.\n",
    "\n",
    "<li> <a href=\"https://pytorch.org/tutorials/\">Pytorch</a> is useful for implementation of RNN. For tutorial on setting up and training RNN, we recommend <a href=\"https://www.kaggle.com/code/kanncaa1/recurrent-neural-network-with-pytorch\">Kaggle tutorial</a> and <a href=\"https://github.com/gyyang/nn-brain/blob/master/RNN_tutorial.ipynb\">Guangyu Robert Yang's tutorial</a> as helpful reference for starting point.\n",
    "\n",
    "<li> Plot the training and testing loss. Do you run into the issue of underfitting or overfitting? How can you identify them? How would you solve them?\n",
    "\n",
    "<li> RNN architecture can have huge impact on predicting neural responses. What do you think would be a good network architecture choice? What are the benefits and limitations of each network architecture?\n",
    "    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
